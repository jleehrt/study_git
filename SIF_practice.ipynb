{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIF_practice.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/FjO4sfl32DZLVHOlnlEz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOf-2TgTkS7X"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jplBYNgTkoHp"
      },
      "source": [
        "SIF를 이용한 문장 임베딩 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNwJDKvLksGR"
      },
      "source": [
        "from IPython.display import Image\r\n",
        "import os\r\n",
        "root_dir = \"/gdrive/My Drive/NLP\"\r\n",
        "Image(os.path.join(root_dir, \"SIF.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJR2LkCxlgRo"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "def getWordmap(word_emb_file, dims):\r\n",
        "    '''\r\n",
        "    단어 임베딩 파일을 읽어서 딕셔너리에 저장해주는 함수\r\n",
        "\r\n",
        "    :param word_emb_file: 단어 임베딩 파일\r\n",
        "    :param dims: 단어 벡터의 크기\r\n",
        "    :return: work2idx: 단어에 대응되는 인덱스 삾을 저장한 딕셔너리\r\n",
        "            embedding: 단어 인덱스에 대응되는 단어 벡터 값을 저장한 딕셔너리\r\n",
        "    '''\r\n",
        "    print(\"Embedding file Loading...\")\r\n",
        "    word2idx = {}\r\n",
        "\r\n",
        "    f = open(word_emb_file, 'r')\r\n",
        "    lines = f.readlines()\r\n",
        "    embedding = np.zeros(shape=(len(lines), dims))\r\n",
        "    for (idx, line) in enumerate(tqdm(lines)):\r\n",
        "        line = line.split()\r\n",
        "        word, vector = line[0], [float(e) for e in line[1:]]\r\n",
        "        word2idx[word] =idx\r\n",
        "        embedding[idx] = vector\r\n",
        "    return word2idx, embedding\r\n",
        "\r\n",
        "def prepare_data(indexing_sentences):\r\n",
        "    '''\r\n",
        "    인덱싱된 문장 리스트를 고정 길이의 array로 표현하기 위한 함수\r\n",
        "\r\n",
        "    :param indexing_sentences: 인덱스로 치환된 문장 리스트\r\n",
        "    :return: sentence_matrix: \"문장 수 X 최대 문장 길이\"로 표현된 인덱싱된 문장 array\r\n",
        "            sentences_mask: 각 문장 array의 요소 유무에 따른 mask array\r\n",
        "            ex - [2, 54, 1, 8, 0, 0] ==> [1, 1, 1, 1, 0, 0]\r\n",
        "    '''\r\n",
        "    lengths = [len(s) for s in indexing_sentences]\r\n",
        "    n_samples = len(indexing_sentences)\r\n",
        "    maxlen = np.max(lengths)\r\n",
        "    sentences_matrix = np.zeros((n_samples, maxlen)).astype('int32')\r\n",
        "    sentences_mask = np.zeros((n_samples, maxlen)).astype('float32')\r\n",
        "    for idx, s in enumerate(indexing_sentences):\r\n",
        "        sentences_matrix[idx, :lengths[idx]] = s\r\n",
        "        sentences_mask[idx, :lengths[idx]] = 1.\r\n",
        "    sentences_mask = np.asarray(sentences_mask, dtyep = 'float32')\r\n",
        "    return sentences_matrix, sentences_mask\r\n",
        "\r\n",
        "def lookupIDX(word2idxm, word):\r\n",
        "    word = word.lower()\r\n",
        "\r\n",
        "    if word in word2idx.keys():\r\n",
        "        return word2idx[word]\r\n",
        "    elif 'unk' in word2idx:\r\n",
        "        return word2idx['unk']\r\n",
        "    else:                                   #\r\n",
        "        return len(word2idx) - 1            # 이 두줄은 안정성을 위해 사용되었다고 한다.\r\n",
        "\r\n",
        "def getSeq(sentence, word2idx):\r\n",
        "    '''\r\n",
        "    단어로 이루어진 문장 리스트를 인덱스로 치환하기 위한 함수\r\n",
        "\r\n",
        "    :param sentence: 단어로 이루어진 문장 리스트\r\n",
        "    :param word2idx: word2idx 딕셔너리\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    sentence = sentence.split()\r\n",
        "    indexing_sentence = []\r\n",
        "    for word in sentence:\r\n",
        "        indexing_sentence.append(lookipIDX(word2idx.word))\r\n",
        "    return indexing_sentence\r\n",
        "\r\n",
        "def senteces2idx(sentences, word2idx):\r\n",
        "    '''\r\n",
        "    자연어 문장 리스트를 인덱스로 치환 후, 고정 길이 array로 변환해주는 함수\r\n",
        "\r\n",
        "    :param sentences: 자연어로 이루어진 문장 리스트\r\n",
        "    :param word2idx: word2idx 딕셔너리\r\n",
        "    :return: sentences_martrix: 인덱스로 치환된 문장 array\r\n",
        "            sentences_maskL 각 문장 array의 요소 유무에 따른 mask array\r\n",
        "    '''\r\n",
        "    indexing_sentences = []\r\n",
        "    for sentece in sentences:\r\n",
        "        indexing_sentences.append(getSeq(sentence, word2idx))\r\n",
        "    sentences_matrux, sentences_mask = prepare_data(indexing_sentences)\r\n",
        "\r\n",
        "    return sentences_martrix, sentences_mask\r\n",
        "\r\n",
        "def getWordWeight(weightfile, rate = 1e-3):\r\n",
        "    '''\r\n",
        "\r\n",
        "    :param weightfile: 단어 빈도 수가 저장된 파일\r\n",
        "    :param rate: 빈도 수에 따른 가중치 계산에 필요한 비율 값\r\n",
        "    :retuen:\r\n",
        "    '''\r\n",
        "    word2weight = {}\r\n",
        "    with open(weightfile) as f:\r\n",
        "        lines = f.readlines()\r\n",
        "    sum_weights = 0\r\n",
        "    print(\"Weight file Loading...\")\r\n",
        "    for line in tqdm(lines):\r\n",
        "        # 집 68451\r\n",
        "        line = line.strip(), split()\r\n",
        "\r\n",
        "        assert len(line) == 2\r\n",
        "        word, weight = line\r\n",
        "        word2weight[word] = float(weight)\r\n",
        "        sum_weights += float(weight)\r\n",
        "\r\n",
        "    for word, weight in word2weight.items():\r\n",
        "        word2weight[word] = rate / (rate + weight/sum_weights)\r\n",
        "\r\n",
        "    return word2weight\r\n",
        "\r\n",
        "def getWeight(word2idx, word2weight):\r\n",
        "    '''\r\n",
        "    word2idx 딕셔너리와 word2weight 딕셔너리를 기반으로 idx2weight 딕셔너리 생성\r\n",
        "\r\n",
        "    :param word2idx:\r\n",
        "    :param word2weight:\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    print(\"Make idx2weight...\")\r\n",
        "    idx2weight = {}\r\n",
        "    for word, idx in tqdm(word2idx, items()):\r\n",
        "        if word in word2weight:\r\n",
        "            idx2weight[idx] = word2weight[word]\r\n",
        "        else:\r\n",
        "            idx2weight[idx] = 1.0\r\n",
        "    return idx2weight\r\n",
        "\r\n",
        "def seq2weight(indexing_sentences, sentences_mask, idx2weight):\r\n",
        "    '''\r\n",
        "    입력 문장을 구성하는 단어의 가중치를 계산하는 함수\r\n",
        "    :param indexing_sentences:\r\n",
        "    :param sentences_mask:\r\n",
        "    :param idx2weight:\r\n",
        "    :return: weight: 문장 리스트의 가중치를 저장한 arrray\r\n",
        "            ex - indexing_sentences = [[1, 2, 3, 4, 0, 0], [...], ...]\r\n",
        "            sentence_mask = [[1, 1, 1, 1, 0, 0], [...], ...]\r\n",
        "            idx2weight = {1:0.1, 2:0.2, 3:0.3, ...}\r\n",
        "\r\n",
        "            weight = [[0.1, 0.2, 0.3, 0.4, 0, 0], [...], ...]\r\n",
        "    '''\r\n",
        "    weight = np.zeros(indexing_sentence.shape, dtype = np.float32)\r\n",
        "    for i in range(indexing_sentences,shape[0]):\r\n",
        "        for j in range(indexing_sentences.shaple[1]):\r\n",
        "            if sentence_mask[i, j] > 0 and indexing_sentences[i, j] >= 0:\r\n",
        "                weight[i, j] = idx2weight[indexing_sentences[i, j]]\r\n",
        "    weight = np.asarray(weight, dtype = 'float32')\r\n",
        "    return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMzVaHC52_U4"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.decomposition import TruncatedSVD\r\n",
        "\r\n",
        "def get_weighted_average(idx2embedding, indexing_sentences, sentence_weights):\r\n",
        "    '''\r\n",
        "    가중치를 반영한 단어 벡터 열을 생성하기 위한 함수\r\n",
        "\r\n",
        "    :param idx2embedding: word_embedding[i, :] i번째 단어 벡터를 저장한 딕셔너리\r\n",
        "    :param indexing_sentences: indexing_sentences[i, :] 인데스로 치환된 고정길이의 i번째 문장\r\n",
        "    :param sentence_weights: sentences_weights[i, :] i번째 문장을 구성하는 단어의 중요도에 따른 가중치\r\n",
        "    :return: emb[i, :] i번째 문장을 구성하는 단어 가중치 X 단어 벡터를 저장한 array\r\n",
        "    '''\r\n",
        "    n_samples = indexing_sentences.shape[0]\r\n",
        "    sentence_embedding_matrix = np.zeros((n_samples, idx2embedding.shape[1]))\r\n",
        "    for i in range(n_samples):\r\n",
        "        sentence_embedding_matrix[i, :] = sentence_weights[i, :].dot(idx2embedding[indexing_sentences[i, :], :]) / np.count_nonzero(sentences_weights[i, :])\r\n",
        "    return sentence_embedding_matrix\r\n",
        "\r\n",
        "def compute_pc(sentence_embedding_matrix):\r\n",
        "    '''\r\n",
        "    입력 문장들의 벡터를 기반으로 공통 성분을 계산하는 함수\r\n",
        "    :param sentence_embedding_matrix: sentence_embedding_matrix[i, :] i번째 문장 벡터\r\n",
        "    :return component_ 전체 문장을 기반으로 계산된 공통 정보 벡터\r\n",
        "    '''\r\n",
        "    svd = TruncatedSVD(n_components = 1, n_iter = 7)\r\n",
        "    svd.fit(sentence_embedding_matrix)\r\n",
        "    return svd.components_\r\n",
        "\r\n",
        "def remove_pc(sentence_embedding_matrix):\r\n",
        "    '''\r\n",
        "    파라미터로 입력받은 문장 벡터 array에서 전체 문장 공통 성분을 제거하기 위한 함수\r\n",
        "    :param sentence_embedding_matrix[i, :] i 번째 문장 벡터\r\n",
        "    :return: 문장 공통 성분이 제거된 문장 벡터들의 array\r\n",
        "    '''\r\n",
        "    pc = compute_pc(sentence_embedding_matrix)\r\n",
        "\r\n",
        "    refine_sentence_embedding_matrix = sentence_embedding_matrix - sentence_embedding_matrix.dot(pc.transpose()) * pc\r\n",
        "    \r\n",
        "    return refine_sentence_embedding_matrix\r\n",
        "\r\n",
        "def SIF_embedding(idx2embedding, indexing_sentences, sentence_weights):\r\n",
        "    '''\r\n",
        "    :param idx2embedding:\r\n",
        "    :param indexing sentences:\r\n",
        "    :param sentence_weights:\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "    emb = get_weighted_average(idx2embedding, indexing_sentences, sentence_weights)\r\n",
        "    emb = remove_pc(emb)\r\n",
        "    return emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfTdJhz2DM55"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "def cos_sim(A, B):\r\n",
        "    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    root_dir = \"/gdrive/My Drive/NLP\"\r\n",
        "    flags = {\r\n",
        "        \"word_embedding_file_path\": os.path.join(root_dir, \"glove.6B.300d.txt\"),\r\n",
        "        \"word_weight_file_path\": os.path.join(root_dirm \"word_count.txt\"),\r\n",
        "        \"weight\": 1e-3,\r\n",
        "        \"word_dims\": 300\r\n",
        "    }\r\n",
        "\r\n",
        "    sentences = ['i like an apple', 'My favorite fruit is an apple', 'i love my dog', 'i really like a dog']\r\n",
        "\r\n",
        "    # load word vectors\r\n",
        "    (word2idxm idx2embedding) = getWordmap(flags[\"word_embedding_file_path\"], flags[\"word_dims\"])\r\n",
        "    # load word weights\r\n",
        "    word2weight = getWordWeight(flags[\"word_weight_file_path\"], flags[\"weight\"]) # word2weight['str'] is the weight for t\r\n",
        "    idx2weight = getWeight(word2idx, word2weight)\r\n",
        "    # load sentences\r\n",
        "    sentences_matrix, sentences_mask = sentences2idx(sentences, word2idx)\r\n",
        "    sentence_weights = seq2weight(sentences_matrix, sentences_mask, idx2weight)\r\n",
        "\r\n",
        "    # get SIF embedding\r\n",
        "    sentence_embedding_martrix = SIF_embedding(idx2embedding, sentences_martrixm sentence_weights)\r\n",
        "\r\n",
        "    for i in range(len(sentence_embedding_martrix)):\r\n",
        "        for j in range(len(sentence_embedding_martrix)):\r\n",
        "            if j > i:\r\n",
        "                sim = cos_sim(sentence_embedding_martrix[i], sentence_embedding_martrix[j])\r\n",
        "                print(\"\\nCos similarity score between '{}' and '{}' : {}\\n\".format(sentences[i], sentences[j], sim))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}